{"cells":[{"cell_type":"markdown","source":"---\ntitle: 线性回归的原理分析与多种Python实现\nseries: ml\nupdate_date: 2019-08-27T15:39:00\ncreate_date: 2019-08-27T15:39:00\nexcerpt: \"本文简要概括线性回归的各种Python实现，线性回归作为机器学习的入门算法具有异常重要的意义，经常用来理解机器学习中的各种重要概念。不仅如此，线性回归还是很多其他经典算法的基础，大量线性或者非线性模型都由线性回归发展而来，理解线性回归的实现，对实现其他算法具有基础性意义。\"\ntype: post\n---","metadata":{"deepnote_cell_type":"markdown","cell_id":"00000-73f8de36-7440-41b0-9bbc-99b6c5fc8e2a","output_cleared":false}},{"cell_type":"markdown","source":"- - -\n\n${toc}\n\n- - -","metadata":{"deepnote_cell_type":"markdown","cell_id":"00001-7fe603b1-39c7-488c-85c7-e94f3aee3a92","output_cleared":false}},{"cell_type":"markdown","source":"本文简要概括线性回归的各种Python实现，线性回归作为机器学习的入门算法具有异常重要的意义，经常用来理解机器学习中的各种重要概念。不仅如此，线性回归还是很多其他经典算法的基础，大量线性或者非线性模型都由线性回归发展而来，理解线性回归的实现，对实现其他算法具有基础性意义。","metadata":{"deepnote_cell_type":"markdown","cell_id":"00002-dec59643-d24a-487f-8898-9c556c26af1b","output_cleared":false}},{"cell_type":"markdown","source":"### 线性回归的基本原理\n\n$$y = \\sum_j\\beta_j x_j$$\n\n其中，$y$是因变量或者响应变量，${x_1...x_j}$是自变量或者输入变量，${\\beta_1...\\beta_j}$是待估计的函数参数。\n\n线性回归是关于上述线性函数参数的估计方法，或者也可以说是关于给定线性模型这个机器的学习过程。\n\n这里的线性模型是指函数相对于函数参数是线性的，实际上上述模型的估计可以用数据矩阵表达为\n\n$$\\hat Y = X\\hat \\beta$$\n\n这里$\\hat Y$是预测值，$X$是已知的输入数据，而$\\hat \\beta$是估计的函数参数。\n\n线性回归有多种估计（优化）方法。\n\n1. 最小二乘法，或者Least squares estimates。从统计角度讲，其实就是最大似然性估计。目标函数如下\n\n    $$\\mathop{\\operatorname{argmin}}\\limits_{\\hat{\\beta}}(Y - \\hat Y)^T(Y - \\hat Y)$$\n    \n    在最优解下，$\\hat Y$其实就是似然概率$Pr(Y|X)$的期望$E(Y|X)$\n    \n    已经知道，最优解的矩阵形式为，\n    $$\\hat \\beta = (X^TX)^{-1} X^TY$$\n    \n    由于目标函数是二阶凸函数，梯度下降的收敛速度很快，而且能够找到全局最优解。\n    \n\n2. 当然还有更复杂一点的贝叶斯线性回归，或者更准确说叫最大事后概率估计，这个下次再讲。","metadata":{"deepnote_cell_type":"markdown","cell_id":"00003-9adca069-a893-4bba-9f16-c2596020c40e","output_cleared":false}},{"cell_type":"markdown","source":"### 线性回归的基本实现","metadata":{"deepnote_cell_type":"markdown","cell_id":"00004-5d88cdeb-e933-4c24-a214-6f755afc7db1","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00005-cd2d11a6-5de5-4cef-ac21-efc440a7c651","output_cleared":false,"source_hash":"3bc5976f","execution_start":1606267521072,"execution_millis":533},"source":"import numpy as np\nfrom sklearn import metrics","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00006-1ce07e81-7858-4ec0-858b-8452b4c9f8f1","output_cleared":false,"source_hash":"a37ab4b6","execution_start":1606267521610,"execution_millis":6},"source":"class LinearRegression:\n    # Optimize B directly with the given analytical solution in previous section\n    def train_in_batch(self, X, y):\n        # prepend bias column\n        X = np.insert(X, 0, 1, axis=1)\n        X_tran = np.transpose(X) # (M + 1) x N\n        X_square_inverse = np.linalg.inv(X_tran.dot(X)) # (M + 1) x M\n        # check dimension match: (M + 1) x (M + 1) x (M + 1) x N x N x 1 = (M + 1) x 1 (correct)\n        self.B = X_square_inverse.dot(X_tran.dot(y))\n        # training mean squared error\n        y_predict = self.predict(X)\n        print(\"Training MSE: \", metrics.mean_squared_error(y, y_predict))\n        print(\"Training variance score: \", metrics.explained_variance_score(y, y_predict))\n        print(\"model parameters: \", self.B)\n    \n    # test on new data in batch\n    def predict(self, new_X):\n        # check dimension match: P x (M + 1) x (M + 1) x 1 = P x 1, P: number of new samples, >= 1 \n        predicted_y = new_X.dot(self.B)\n        return predicted_y","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 模型的基本测试","metadata":{"deepnote_cell_type":"markdown","cell_id":"00007-647703fc-348c-4103-b07b-b6b7c41d90de","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00008-afda60d1-68de-47d1-b0ad-3647bf7c8997","output_cleared":false,"source_hash":"916419fd","execution_start":1606267521620,"execution_millis":62},"source":"# Load benchmark dataset\nfrom sklearn import datasets\nX, y = datasets.load_diabetes(return_X_y=True)\nX.shape, y.shape","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"((442, 10), (442,))"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00009-d29d0946-27c0-49c4-b062-9130e4048115","output_cleared":false,"source_hash":"cc3f19e6","execution_start":1606267521724,"execution_millis":0},"source":"# Run model\ndef test_basic_model():\n    model = LinearRegression()\n    model.train_in_batch(X, y)\ntest_basic_model()","execution_count":4,"outputs":[{"name":"stdout","text":"Training MSE:  2859.6903987680657\nTraining variance score:  0.5177494254132934\nmodel parameters:  [ 152.13348416  -10.01219782 -239.81908937  519.83978679  324.39042769\n -792.18416163  476.74583782  101.04457032  177.06417623  751.27932109\n   67.62538639]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 线性回归的基本假设\n\n线性回归具有唯一解的条件是每个输入特征列都是线性独立的，换句话说，输入矩阵的列之间没有相关性。上面的实现没有考虑这种情况，所以在下面的test case里会报错。","metadata":{"deepnote_cell_type":"markdown","cell_id":"00010-4beda037-4792-431c-8223-0d0c67cb8759","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00011-1ec30b0c-afba-4675-8ab8-6bb20ebd5e80","output_cleared":false,"source_hash":"f4ab9248","execution_start":1606267521725,"execution_millis":0},"source":"def test_correlated_input():\n    # add a new column at column 5 that is fully correlated with the original column 8\n    correlated_X = np.insert(X, 5, X[:, 8] * 2, axis=1)\n    model = LinearRegression()\n    model.train_in_batch(correlated_X, y)\ntry:\n    test_correlated_input()\nexcept np.linalg.LinAlgError as e:\n    print(\"Error occurs because input is a\", e)","execution_count":5,"outputs":[{"name":"stdout","text":"Error occurs because input is a Singular matrix\n","output_type":"stream"}]},{"cell_type":"markdown","source":"当输入数据的数量大于特征的数量时，结果可能不稳定。表现为有些系数很大，而另外一些特别小。","metadata":{"deepnote_cell_type":"markdown","cell_id":"00012-e1e7bb70-6e6c-4325-a273-ce48df6f370c","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00013-9bec1af1-def4-4b4b-82cc-2b14a1b584a4","output_cleared":false,"source_hash":"f0874179","execution_start":1606267521725,"execution_millis":4},"source":"def test_rank_deficient_input():\n    # make sure the number of rows are less than number of columns, we have 10 columns and 8 rows in the following case\n    new_N = 8\n    deficient_X = X[:new_N,:]\n    deficient_y = y[:new_N]\n    model = LinearRegression()\n    model.train_in_batch(deficient_X, deficient_y)\ntry:\n    test_rank_deficient_input()\nexcept np.linalg.LinAlgError as e:\n    print(\"Error occurs because input is a\", e)","execution_count":6,"outputs":[{"name":"stdout","text":"Training MSE:  155884507.42055053\nTraining variance score:  -66885.81595119795\nmodel parameters:  [ -10240.   44544.  -65536. -131072.  196608.  524288. -262144. -655360.\n -196608. -262144.  -65536.]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"当两个特征高度相关但不完全相关时，结果也可能不稳定。表现为有些模型系数很大，而另外一些特别小。","metadata":{"deepnote_cell_type":"markdown","cell_id":"00014-c34c39e3-1e36-407a-8fcc-b3fb9518e1b5","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00015-510822fc-499c-4d23-8180-d86fdf822165","output_cleared":false,"source_hash":"5fb50042","execution_start":1606267521737,"execution_millis":5},"source":"def test_highly_correlated_input():\n    # feature #5 and #9 are highly correlated\n    highly_correlated_data = X[:, 8] * 2\n    highly_correlated_data[-1] = highly_correlated_data[-1] + 0.3\n    highly_correlated_X = np.insert(X, 5, highly_correlated_data, axis=1)\n    highly_correlated_y = y\n    model = LinearRegression()\n    model.train_in_batch(highly_correlated_X, highly_correlated_y)\ntry:\n    test_rank_deficient_input()\nexcept np.linalg.LinAlgError as e:\n    print(\"Error occurs because input is a\", e)","execution_count":7,"outputs":[{"name":"stdout","text":"Training MSE:  155884507.42055053\nTraining variance score:  -66885.81595119795\nmodel parameters:  [ -10240.   44544.  -65536. -131072.  196608.  524288. -262144. -655360.\n -196608. -262144.  -65536.]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 线性回归的统计推断\n\n线性回归的统计推断主要是通过统计推断的方式来验证线性回归函数参数的假设概率分布模型，以及最优解的相关统计性质，比如信度区间。\n线性回归的统计推断基于以下假设（非常重要）,\n\n1. $y_i$之间没有相关性的，并且$Pr(y_i)$的方差恒定为$\\sigma^2$， $\\sigma^2$可以通过$y_i$和$\\hat y_i$的统计方差来估计: $\\hat \\sigma^2 = \\frac{1}{N-M-1}\\sum_{i=1}^{N} (y_i-\\hat y_i)^2$。\n\n2. $E(Y|X) + \\epsilon$是$y_i$的估计模型，$E(Y|X)$为关于$X$的线性模型,并且$y_i$服从正态分布$\\epsilon \\sim N(0, \\sigma^2)$。\n3. $x_i$已知，并且服从中心极限定理，意味着在$N \\sim \\infty$，输入变量服从正态分布。\n\n基于假设1和2，我们可以有以下推断，\n\n1. $(N-M-1)\\hat \\sigma^2$服从chi-squared概率分布$(N-M-1)\\hat \\sigma^2 \\sim \\sigma^2\\chi^2_{N-M-1}$。\n2. $\\hat \\beta$服从多变量正态分布$\\hat \\beta \\sim N(\\beta, (X^TX)^{-1}\\sigma^2)$，并且变量之间相互独立。\n3. $\\hat \\beta$和$\\hat \\sigma^2$相互独立。","metadata":{"deepnote_cell_type":"markdown","cell_id":"00016-42bf843a-33a3-4e57-82a9-223d0258e560","output_cleared":false}},{"cell_type":"markdown","source":"### 线性回归的统计测试\n\n在实际应用中，我们通常需要在建模前或者建模后进行一些统计测试，来帮助我们确定最合适的线性回归模型。相关问题可能包括，\n\n1. 是否有高度相关的特征变量，如果是，是否需要排除？\n2. 怎样评估不同函数参数（对应不同特征变量）对模型的影响程度？（其实就是特征变量的重要性）\n3. 怎样在统计意义下，比较不同参数选择下模型的表现？","metadata":{"deepnote_cell_type":"markdown","cell_id":"00017-f9d738ec-1bc0-4a92-a90e-cd6421cebe05","output_cleared":false}},{"cell_type":"markdown","source":"#### 相关性表\n\n研究输入变量两两间的相关性，以及与相应变量之间的相关性。相关性系数的公式如下。\n\n$Cor(x_i, x_j) = \\frac{cov(x_i, x_j)}{\\sigma_{x_i}, \\sigma_{x_j}}$\n\n$cov(x_i, x_j) = \\mu_{x_i x_y} - \\mu_{x_i}\\mu_{x_j}$\n\n对于矩阵$X=[x_1, ..., x_j]$来说，\n\n共方差矩阵(covariance matrix)为\n\n$cov(X, X) = E(X^TX) - E(X)E(X^T)$\n\n相关性系数矩阵(correlation matrix)为\n\n$cor(X, X) = (diag(cov(X, X))^{-\\frac{1}{2}}) cov(X, X) (diag(cov(X, X))^{-\\frac{1}{2}})$","metadata":{"deepnote_cell_type":"markdown","cell_id":"00018-f6ece6b6-160d-4bf5-a80d-114e86d58206","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00019-f7ee776a-02b1-419c-a3d5-a755d22c3c6d","output_cleared":false,"source_hash":"2d1798df","execution_millis":1,"execution_start":1606268067081},"source":"# Implementation of correlation coefficient matrix\ndef cov(x1, x2):\n    return np.mean(np.multiply(x1, x2)) - np.mean(x1) * np.mean(x2)\n\ndef cor(x1, x2):\n    return cov(x1, x2) / (np.std(x1) * np.std(x2))\n\ndef zscore(X):\n    mean_matrix = np.tile(np.mean(X, axis=0), (X.shape[0],1))\n    std_matrix = np.tile(np.std(X, axis=0), (X.shape[0],1))\n    return np.divide(X - mean_matrix, std_matrix)\n\ndef cov_matrix(X1, X2):\n    zscore_X1 = zscore(X1)\n    zscore_X2 = zscore(X2)\n    N = zscore_X1.shape[0]\n    return np.transpose(zscore_X1).dot(zscore_X2)/N - np.mean(zscore_X1, axis=0) * np.transpose(np.mean(zscore_X1, axis=0))\n\ndef diagonal_matrix(X):\n    return np.diag(np.diag(X))\n\ndef cor_matrix(X1, X2):\n    cov_m = cov_matrix(X1, X2)\n    C = np.sqrt(np.linalg.inv(diagonal_matrix(cov_m)))\n    return C.dot(cov_m).dot(C)  ","execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00020-7e0bd839-c9c3-423c-8d59-f52f0ede8cad","output_cleared":false,"source_hash":"62ab8d68","execution_millis":18,"execution_start":1606268067551},"source":"def test_cor_matrix_implementation():\n    new_y = np.reshape(y, (y.shape[0], 1))\n    Xy = np.append(X, new_y, axis=1)\n    return np.allclose(cor_matrix(Xy, Xy), np.corrcoef(np.transpose(Xy)))\ntest_cor_matrix_implementation()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"##### 根据相关系数表来去掉完全相关的输入变量","metadata":{"deepnote_cell_type":"markdown","cell_id":"00021-5abc09b4-48c4-45e7-a7f5-b8fd31c4882d","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00022-8c17783f-c77c-4c8b-9f22-0981c4ce9c68","output_cleared":false,"source_hash":"29d37f67","execution_millis":10,"execution_start":1606268069921},"source":"# implementation to detect correlated input\ndef is_correlated_input(X):\n    return np.linalg.matrix_rank(np.transpose(X).dot(X)) != X.shape[1]\nhighly_correlated_data = X[:, 8] * 2\nhighly_correlated_data[-1] = highly_correlated_data[-1] + 0.3\nhighly_correlated_X = np.insert(X, 5, highly_correlated_data, axis=1)\nfully_correlated_X = np.insert(X, 5, X[:, 8] * 2, axis=1)\nprint(\"original input is full rank: \", not is_correlated_input(X))\nprint(\"highly correlated input is full rank: \", not is_correlated_input(highly_correlated_X))\nprint(\"fully correlated input is full rank: \", not is_correlated_input(fully_correlated_X))","execution_count":17,"outputs":[{"name":"stdout","text":"original input is full rank:  True\nhighly correlated input is full rank:  True\nfully correlated input is full rank:  False\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00023-34de50bf-4767-42ca-86cb-a326e7631f9f","output_cleared":false,"source_hash":"53af931d","execution_millis":2,"execution_start":1606268072365},"source":"# correlation coefficient matrix for highly correlated input\nm = cor_matrix(highly_correlated_X, highly_correlated_X)\nprint(np.array_str(m, precision=2))\n# output shows at column 5, row 9, the correlation coefficient is 0.99, yet the matrix is still non-singular","execution_count":18,"outputs":[{"name":"stdout","text":"[[ 1.    0.17  0.19  0.34  0.26  0.26  0.22 -0.08  0.2   0.27  0.3 ]\n [ 0.17  1.    0.09  0.24  0.04  0.14  0.14 -0.38  0.33  0.15  0.21]\n [ 0.19  0.09  1.    0.4   0.25  0.43  0.26 -0.37  0.41  0.45  0.39]\n [ 0.34  0.24  0.4   1.    0.24  0.38  0.19 -0.18  0.26  0.39  0.39]\n [ 0.26  0.04  0.25  0.24  1.    0.52  0.9   0.05  0.54  0.52  0.33]\n [ 0.26  0.14  0.43  0.38  0.52  1.    0.32 -0.37  0.61  0.99  0.46]\n [ 0.22  0.14  0.26  0.19  0.9   0.32  1.   -0.2   0.66  0.32  0.29]\n [-0.08 -0.38 -0.37 -0.18  0.05 -0.37 -0.2   1.   -0.74 -0.4  -0.27]\n [ 0.2   0.33  0.41  0.26  0.54  0.61  0.66 -0.74  1.    0.62  0.42]\n [ 0.27  0.15  0.45  0.39  0.52  0.99  0.32 -0.4   0.62  1.    0.46]\n [ 0.3   0.21  0.39  0.39  0.33  0.46  0.29 -0.27  0.42  0.46  1.  ]]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00024-1aea85f2-3a2c-4828-b5cd-a6d59d13068d","output_cleared":false,"source_hash":"9835336d","execution_millis":2,"execution_start":1606268074419},"source":"# implementation to find fully correlated columns\ndef find_correlated_columns(X):\n    m = cor_matrix(X, X)\n    used_cols = set()\n    cor_groups = []\n    for i in range(0, m.shape[0]):\n        if i in used_cols:\n            continue\n        else:\n            indices = list(filter(lambda x: x != i, np.argwhere(m[i,:] == 1).flatten()))\n            if len(indices) > 0:\n                cor_group = (i,) + tuple(indices)\n                cor_groups.append(cor_group)\n                used_cols = used_cols | set(cor_group)\n    rest_cols = set(range(0, m.shape[1])) - used_cols\n    for j in rest_cols:\n        indices = list(filter(lambda x: x != j, np.argwhere(m[:,j] == 1).flatten()))\n        if len(indices) > 0:\n            cor_group = (j,) + tuple(indices)\n            cor_groups.append(cor_group)\n            used_cols = used_cols | set(cor_group)\n    return cor_groups","execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00025-6b8f62d9-c42d-4f49-bd0a-bf598301977f","output_cleared":false,"source_hash":"ad961e94","execution_millis":14,"execution_start":1606268076057},"source":"# implementation to find fully correlated columns and remove redundant columns\ndef remove_correlated_columns(X):\n    cor_groups = find_correlated_columns(X)\n    print('Found correlated column groups: ', cor_groups)\n    remove_columns = np.array(list(map(lambda group: group[1:], cor_groups))).flatten().tolist()\n    print('Removed correlated columns: ', remove_columns)\n    return np.delete(X, remove_columns, 1)\n    \nprint(remove_correlated_columns(fully_correlated_X).shape)","execution_count":20,"outputs":[{"name":"stdout","text":"Found correlated column groups:  [(5, 9)]\nRemoved correlated columns:  [9]\n(442, 10)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 函数系数表\n\n函数系数表统计各个系数（对应各个输入变量）的值，标准差，Z-score（这里的z-score是对于函数参数$\\beta_j$而言，和前面的输入变量的z-score不同），以及系数对模型影响（去掉或者添加某一系数是否会显著改变模型表现）的显著性$p$值。\n\n函数系数$\\beta_j$的z-score可以通过与求$x_j$的z-score相似的方法得出，只是$\\beta_j$服从不同的概率分布（参考统计推断小结）。\n\n$z_{j} = \\frac{\\hat \\beta_j}{\\hat \\sigma \\sqrt v_j}$\n\n其中$v_j$是矩阵$(X^TX)^{-1}$的对角元素，并且这里假定$\\beta_j = 0$ （代表缺少这个参数时对函数表现的影响）。\n\n标准化的$\\beta_j$即$z_j$服从student-t distribution\n\n$z_j \\sim t_{N-M-1} \\sim N(0, 1) (N \\gg M)$\n\n其中$M$是函数参数的个数。\n\n已知$N$, $M$，通过student-t概率分布函数（没有计算机的时候人们通常查表来得到相应的数据！），我们可以得到在概率为$5 \\% $（$95 \\% $信度水平）的情况下，拒绝假设（意味着$\\beta_j$能显著改善模型表现）需要的最小z-score值。或者给定z-score，我们可以求出在$95\\%$信度水平下的显著性$p$值。\n\n由于在实际应用中，$N \\gg M$比较容易满足，所以$\\beta_j$的信度区间，\n\n$(\\hat \\beta_j - z^{(1-\\alpha)}v_j^{\\frac{1}{2}}\\hat \\sigma, \\hat \\beta_j + z^{(1-\\alpha)}v_j^{\\frac{1}{2}}\\hat \\sigma)$\n\n在$\\alpha = 95 \\% $的显著性水平下，约等于\n\n$(\\hat \\beta_j - 2 se(\\hat \\beta_j), \\hat \\beta_j + 2 se(\\hat \\beta_j))$","metadata":{"deepnote_cell_type":"markdown","cell_id":"00026-05b5bdc3-7137-47b2-afd3-297b6242f50b","output_cleared":false}},{"cell_type":"markdown","source":"#### F测量 (F measurement)\n\n函数系数表给出了单一变量的增减对于模型表现的影响。F测量主要用来测量多个变量的增减对于模型表现的影响。\n\n$F = \\frac{(RSS_0 - RSS_1)/(M_1 - M_0)}{RSS_1 / (N - M_1 - 1)}$\n\n其中，$RSS = \\sum(y_j - \\hat y_j)^2$, 0和1代表两种不同模型，$M_0, M_1$是这两个模型的特征向量各自的长度，假设$M_0 < M_1$。\n\n$F$的取值实际上服从F分布：$F \\sim F_{M_1 - M_0, N - M_1 - 1} \\sim \\chi^2_{M_1 - M_0} (N \\gg M_1)$。\n\n当$M_1 - M_0 = 1$时，$F$的值实际上等价于$z_j$，服从相似的student-t distribution，近似于正态分布。","metadata":{"deepnote_cell_type":"markdown","cell_id":"00027-dcc09613-3507-4d49-afba-f220651f514f","output_cleared":false}},{"cell_type":"markdown","source":"### 完整的线性回归算法实现","metadata":{"deepnote_cell_type":"markdown","cell_id":"00028-b7a69130-de0f-43f9-ac89-178d0195cf01","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00029-218355ec-94f3-43c4-9a80-45d2a3fe1b4a","output_cleared":false,"source_hash":"8c9c9005","execution_millis":6,"execution_start":1606268081269},"source":"from sklearn import metrics\nimport numpy as np\n\nclass LinearRegression:\n    def _find_correlated_columns(self, X):\n        m = cor_matrix(X, X)\n        used_cols = set()\n        cor_groups = []\n        for i in range(0, m.shape[0]):\n            if i in used_cols:\n                continue\n            else:\n                indices = list(filter(lambda x: x != i, np.argwhere(m[i,:] == 1).flatten()))\n                if len(indices) > 0:\n                    cor_group = (i,) + tuple(indices)\n                    cor_groups.append(cor_group)\n                    used_cols = used_cols | set(cor_group)\n        rest_cols = set(range(0, m.shape[1])) - used_cols\n        for j in rest_cols:\n            indices = list(filter(lambda x: x != j, np.argwhere(m[:,j] == 1).flatten()))\n            if len(indices) > 0:\n                cor_group = (j,) + tuple(indices)\n                cor_groups.append(cor_group)\n                used_cols = used_cols | set(cor_group)\n        return cor_groups\n\n    def _remove_correlated_columns(self, X):\n        cor_groups = find_correlated_columns(X)\n        print('Found correlated column groups: ', cor_groups)\n        remove_columns = np.array(list(map(lambda group: group[1:], cor_groups))).flatten().tolist()\n        print('Removed correlated columns: ', remove_columns)\n        return np.delete(X, remove_columns, 1)\n    \n    # Optimize B directly with the given analytical solution in previous section\n    def train_in_batch(self, X, y):\n        X = self._remove_correlated_columns(X)\n        X = np.insert(X, 0, 1, axis=1)\n        X_tran = np.transpose(X) # M x N\n        X_square_inverse = np.linalg.inv(X_tran.dot(X)) # M x M\n        # check dimension match: M x M x M x N x N x 1 = M x 1 (correct)\n        self.B = X_square_inverse.dot(X_tran.dot(y))\n        # training mean squared error\n        y_predict = self.predict(X)\n        print(\"Training MSE: \", metrics.mean_squared_error(y, y_predict))\n        print(\"Training variance score: \", metrics.explained_variance_score(y, y_predict))\n        print(\"model parameters: \", self.B)\n    \n    # test on new data in batch\n    def predict(self, new_X):\n        # check dimension match: P x M x M x 1 = P x 1, P: number of new samples, >= 1 \n        predicted_y = new_X.dot(self.B)\n        return predicted_y","execution_count":21,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"deepnote_notebook_id":"84faaa4f-29f3-47a0-bb17-ce7c2e0fdff3","deepnote_execution_queue":[]}}